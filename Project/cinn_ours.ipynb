{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining our loss function\n",
    "def calculate_loss(transformed_x, scaling_before_exp_list, dataset_length):\n",
    "    \"\"\"\n",
    "    Calculate the Negative log likelyhood loss for the RealNVP model.\n",
    "\n",
    "    Args:\n",
    "    - transformed_x (tensor): Transformed data produced by the RealNVP model.\n",
    "    - scaling_before_exp_list (list): List of scaling_before_exp values for each block.\n",
    "    - dataset_length (int): The length of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - loss (tensor): The calculated loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the first term of the loss (negative log-likelihood term)\n",
    "    first_term = 0.5 * torch.sum(transformed_x**2)\n",
    "\n",
    "    second_term = -torch.sum(\n",
    "        torch.cat(scaling_before_exp_list)\n",
    "    )  # torch.sum(torch.stack(model.scaling_before_exp_list), dim=0)\n",
    "\n",
    "    # Calculate the total loss\n",
    "    loss = (first_term + second_term) / dataset_length\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalCouplingLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, condition_size):\n",
    "        \"\"\"\n",
    "        Initialize a ConditionalCouplingLayer.\n",
    "\n",
    "        Args:\n",
    "        - input_size (int): Total size of the input data.\n",
    "        - hidden_size (int): Size of the hidden layers in the neural networks.\n",
    "        - condition_size (int): Size of the condition vector (e.g., one-hot encoded label size).\n",
    "        \"\"\"\n",
    "        super(ConditionalCouplingLayer, self).__init__()\n",
    "        # Neural networks for the first half of the dimensions\n",
    "        self.fc1 = nn.Linear(input_size // 2 + condition_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # Translation coefficient\n",
    "        self.fc3 = nn.Linear(hidden_size, input_size // 2)\n",
    "        # Scaling coefficient\n",
    "        self.fc4 = nn.Linear(hidden_size, input_size // 2)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        \"\"\"\n",
    "        Forward pass through the ConditionalCouplingLayer.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input data.\n",
    "        - condition (torch.Tensor): Condition vector.\n",
    "\n",
    "        Returns:\n",
    "        - y (torch.Tensor): Transformed data.\n",
    "        - scaling_before_exp (torch.Tensor): Scaling coefficients before the exponential operation.\n",
    "        \"\"\"\n",
    "        # Split the input into two halves\n",
    "        x_a, x_b = x.chunk(2, dim=1)\n",
    "\n",
    "        # Concatenate conditions to the first half\n",
    "        x_a_concat = torch.cat([x_a, condition], dim=1)\n",
    "\n",
    "        # Apply neural network to calculate coefficients\n",
    "        h = F.relu(self.fc1(x_a_concat))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        translation = self.fc3(h)\n",
    "        scaling_before_exp = torch.tanh(self.fc4(h))\n",
    "        scaling = torch.exp(scaling_before_exp)\n",
    "\n",
    "        # Apply the affine transformation\n",
    "        y_b = x_b * scaling + translation\n",
    "\n",
    "        # Concatenate the transformed halves\n",
    "        y = torch.cat([x_a, y_b], dim=1)\n",
    "        return y, scaling_before_exp\n",
    "\n",
    "    def backward(self, y, condition):\n",
    "        \"\"\"\n",
    "        Backward pass through the ConditionalCouplingLayer.\n",
    "\n",
    "        Args:\n",
    "        - y (torch.Tensor): Transformed data.\n",
    "        - condition (torch.Tensor): Condition vector.\n",
    "\n",
    "        Returns:\n",
    "        - x (torch.Tensor): Reconstructed original input.\n",
    "        \"\"\"\n",
    "        # Split the input into two halves\n",
    "        y_a, y_b = y.chunk(2, dim=1)\n",
    "\n",
    "        # Concatenate conditions to the first half\n",
    "        y_a_concat = torch.cat([y_a, condition], dim=1)\n",
    "\n",
    "        # Apply neural network to calculate coefficients (reverse)\n",
    "        h = F.relu(self.fc1(y_a_concat))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        translation = self.fc3(h)\n",
    "        scaling_before_exp = self.fc4(h)\n",
    "        scaling = torch.exp(torch.tanh(scaling_before_exp))\n",
    "\n",
    "        # Reverse the operations to reconstruct the original input\n",
    "        x_a = y_a\n",
    "        x_b = (y_b - translation) / scaling\n",
    "\n",
    "        # Concatenate the reconstructed halves\n",
    "        x = torch.cat([x_a, x_b], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### conditional real NVP class\n",
    "class ConditionalRealNVP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, condition_size, blocks):\n",
    "        \"\"\"\n",
    "        Initialize a ConditionalRealNVP model.\n",
    "\n",
    "        Args:\n",
    "        - input_size (int): Total size of the input data.\n",
    "        - hidden_size (int): Size of the hidden layers in the neural networks.\n",
    "        - condition_size (int): Size of the condition vector (e.g., one-hot encoded label size).\n",
    "        - blocks (int): Number of coupling layers in the model.\n",
    "        \"\"\"\n",
    "        super(ConditionalRealNVP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.condition_size = condition_size\n",
    "        self.blocks = blocks\n",
    "\n",
    "        # List of coupling layers\n",
    "        self.coupling_layers = nn.ModuleList(\n",
    "            [\n",
    "                ConditionalCouplingLayer(input_size, hidden_size, condition_size)\n",
    "                for _ in range(blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # List to store orthonormal matrices\n",
    "        self.orthonormal_matrices = [\n",
    "            self._get_orthonormal_matrix(input_size) for _ in range(blocks)\n",
    "        ]\n",
    "\n",
    "        # List to store scaling_before_exp for each block\n",
    "        self.scaling_before_exp_list = []\n",
    "\n",
    "    def _get_orthonormal_matrix(self, size):\n",
    "        \"\"\"\n",
    "        Generate a random orthonormal matrix.\n",
    "\n",
    "        Args:\n",
    "        - size (int): Size of the matrix.\n",
    "\n",
    "        Returns:\n",
    "        - q (torch.Tensor): Orthonormal matrix.\n",
    "        \"\"\"\n",
    "        w = torch.randn(size, size)\n",
    "        q, _ = torch.linalg.qr(w, \"reduced\")\n",
    "        return q\n",
    "\n",
    "    def forward_realnvp(self, x, condition):\n",
    "        \"\"\"\n",
    "        Forward pass through the ConditionalRealNVP model.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input data.\n",
    "        - condition (torch.Tensor): Condition vector.\n",
    "\n",
    "        Returns:\n",
    "        - x (torch.Tensor): Transformed data.\n",
    "        \"\"\"\n",
    "        scaling_before_exp_list = []\n",
    "        for i in range(self.blocks):\n",
    "            # print(\"x is:\"); print(x)\n",
    "            # print(\"shape of x is:\"); print(x.shape)\n",
    "            x = torch.matmul(x, self.orthonormal_matrices[i])\n",
    "            x, scaling_before_exp = self.coupling_layers[i].forward(x, condition)\n",
    "            scaling_before_exp_list.append(scaling_before_exp)\n",
    "\n",
    "        self.scaling_before_exp_list = scaling_before_exp_list\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, condition):\n",
    "        \"\"\"\n",
    "        Reverse transformations to decode data.\n",
    "\n",
    "        Args:\n",
    "        - z (torch.Tensor): Transformed data.\n",
    "        - condition (torch.Tensor): Condition vector.\n",
    "\n",
    "        Returns:\n",
    "        - z (torch.Tensor): Reconstructed original data.\n",
    "        \"\"\"\n",
    "        for i in reversed(range(self.blocks)):\n",
    "            z = self.coupling_layers[i].backward(z, condition)\n",
    "            z = torch.matmul(z, self.orthonormal_matrices[i].t())\n",
    "        return z\n",
    "\n",
    "    def sample(self, num_samples=1000, conditions=None):\n",
    "        \"\"\"\n",
    "        Generate synthetic samples.\n",
    "\n",
    "        Args:\n",
    "        - num_samples (int): Number of synthetic samples to generate.\n",
    "        - conditions (torch.Tensor): Conditions for generating synthetic samples.\n",
    "\n",
    "        Returns:\n",
    "        - synthetic_samples (torch.Tensor): Synthetic samples.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, self.input_size)\n",
    "            synthetic_samples = self.decode(z, conditions)\n",
    "        return synthetic_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training_the_conditional_nvp model\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "def train_and_validate_conditional_nvp(\n",
    "    model, train_loader, val_loader, num_epochs=10, lr=0.001, print_after=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the ConditionalRealNVP model and evaluate on a validation dataset.\n",
    "\n",
    "    Args:\n",
    "    - model (ConditionalRealNVP): The ConditionalRealNVP model to be trained.\n",
    "    - train_loader (DataLoader): DataLoader for the training dataset.\n",
    "    - val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "    - num_epochs (int): Number of training epochs.\n",
    "    - lr (float): Learning rate for the optimizer.\n",
    "    - print_after (int): Number of epochs after which to print the training and validation loss.\n",
    "\n",
    "    Returns:\n",
    "    - train_losses (list): List of training losses for each epoch.\n",
    "    - val_losses (list): List of validation losses for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []  # List to store training losses\n",
    "    val_losses = []  # List to store validation losses\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        for data, labels in train_loader:\n",
    "            inputs = data\n",
    "            conditions = one_hot(labels, num_classes=model.condition_size).float()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (encoding)\n",
    "            encoded = model.forward_realnvp(inputs, conditions)\n",
    "\n",
    "            # Loss calculation\n",
    "            train_loss = calculate_loss(\n",
    "                encoded, model.scaling_before_exp_list, len(train_loader)\n",
    "            )\n",
    "\n",
    "            # Backward pass (gradient computation)\n",
    "            train_loss.backward()\n",
    "\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += train_loss.item()\n",
    "\n",
    "        # Average training loss for the epoch\n",
    "        average_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader is not None:\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            total_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_data, val_labels in val_loader:\n",
    "                    val_inputs = val_data\n",
    "                    val_conditions = one_hot(\n",
    "                        val_labels, num_classes=model.condition_size\n",
    "                    ).float()\n",
    "\n",
    "                    # Forward pass (encoding) for validation\n",
    "                    val_encoded = model.forward_realnvp(val_inputs, val_conditions)\n",
    "\n",
    "                    # Loss calculation for validation\n",
    "                    val_loss = calculate_loss(\n",
    "                        val_encoded, model.scaling_before_exp_list, len(val_loader)\n",
    "                    )\n",
    "\n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "            # Average validation loss for the epoch\n",
    "            average_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "            # Print training and validation losses together\n",
    "            if (epoch + 1) % print_after == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss}, Validation Loss: {average_val_loss}\"\n",
    "                )\n",
    "\n",
    "            # Append losses to the lists\n",
    "            train_losses.append(average_train_loss)\n",
    "            val_losses.append(average_val_loss)\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        model.train()\n",
    "\n",
    "    print(\"Training complete\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
