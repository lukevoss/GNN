{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from scipy.stats import special_ortho_group\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subnet_constructor(input_size, hidden_size, output_size):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, output_size),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def ortogonal_matrix(dim):\n",
    "    \"\"\"A = torch.normal(mean=torch.zeros((dim,dim)), std=torch.ones((dim,dim))) Q, _ = torch.linalg.qr(A)\n",
    "    if dim == 2:\n",
    "            Q[1,1] = -Q[0,0]\n",
    "    Q[0,1] = -Q[1,0] print(torch.linalg.det(Q)) return Q\"\"\"\n",
    "    Q = special_ortho_group.rvs(dim)\n",
    "    return torch.Tensor(Q)\n",
    "\n",
    "\n",
    "class conditional_coupling_block(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, condition_size):\n",
    "        \"\"\"\n",
    "        Initialize a ConditionalCouplingLayer.\n",
    "\n",
    "        Args:\n",
    "        - input_size (int): Total size of the input data.\n",
    "        - hidden_size (int): Size of the hidden layers in the neural networks.\n",
    "        - condition_size (int): Size of the condition vector (e.g., one-hot encoded label size).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.condition_size = condition_size\n",
    "        self.split1 = math.floor(self.input_size / 2)\n",
    "        self.split2 = self.input_size - self.split1\n",
    "        self.subnet = subnet_constructor(\n",
    "            self.split1 + self.condition_size, self.hidden_size, 2 * self.split2\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond, rev=False):\n",
    "        x1, x2 = x[..., : self.split1], x[..., self.split1 :]\n",
    "        params = self.subnet(torch.cat([x1, cond], -1))\n",
    "        s, t = params[..., : self.split2], params[..., self.split2 :]\n",
    "        s = torch.tanh(s)\n",
    "        ljd = torch.sum(s, -1)\n",
    "        if not rev:\n",
    "            s = torch.exp(s)\n",
    "            x2 = s * x2 + t  # Apply the affine transformation\n",
    "            return torch.cat([x1, x2], -1), ljd\n",
    "        if rev:\n",
    "            s = torch.exp(-s)\n",
    "            x2 = s * (x2 - t)  # Reverse the affine transformation\n",
    "            return torch.cat([x1, x2], -1)\n",
    "\n",
    "\n",
    "class conditional_realNVP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_blocks, condition_size):\n",
    "        \"\"\"\n",
    "        Initialize a ConditionalRealNVP model.\n",
    "\n",
    "        Args:\n",
    "        - input_size (int): Total size of the input data.\n",
    "        - hidden_size (int): Size of the hidden layers in the neural networks.\n",
    "        - condition_size (int): Size of the condition vector (e.g., one-hot encoded label size).\n",
    "        - n_blocks (int): Number of coupling layers in the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_blocks = n_blocks\n",
    "        self.condition_size = condition_size\n",
    "        self.coupling_blocks = nn.ModuleList(\n",
    "            [\n",
    "                conditional_coupling_block(input_size, hidden_size, self.condition_size)\n",
    "                for _ in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.orthogonal_matrices = [\n",
    "            ortogonal_matrix(input_size) for _ in range(n_blocks - 1)\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, cond, rev=False):\n",
    "        if rev:\n",
    "            return self._inverse(x, cond)\n",
    "        return self._forward(x, cond)\n",
    "\n",
    "    def _forward(self, x, cond):\n",
    "        cond = nn.functional.one_hot(\n",
    "            cond.to(torch.int64), num_classes=self.condition_size\n",
    "        )  # TODO: condition gets onehot encoded. Does that work for us?\n",
    "        ljd = torch.zeros((x.shape[0]))\n",
    "        for l in range(self.n_blocks - 1):\n",
    "            x, partial_ljd = self.coupling_blocks[l](x, cond)\n",
    "            ljd += partial_ljd\n",
    "            x = torch.matmul(x, self.orthogonal_matrices[l])\n",
    "        x, partial_ljd = self.coupling_blocks[-1](x, cond)\n",
    "        ljd += partial_ljd\n",
    "        return x, ljd\n",
    "\n",
    "    def _inverse(self, x, cond):\n",
    "        cond = nn.functional.one_hot(\n",
    "            cond.to(torch.int64), num_classes=self.condition_size\n",
    "        )\n",
    "        for l in range(self.n_blocks - 1, 0, -1):\n",
    "            x = self.coupling_blocks[l](x, cond, rev=True)\n",
    "            x = torch.matmul(x, self.orthogonal_matrices[l - 1].T)\n",
    "        x = self.coupling_blocks[0](x, cond, rev=True)\n",
    "        return x\n",
    "\n",
    "    def sample(self, num_samples, cond=None):\n",
    "        samples = []\n",
    "        if cond is None:\n",
    "            for c in range(self.condition_size):\n",
    "                z = torch.normal(\n",
    "                    mean=torch.zeros((num_samples, self.input_size)),\n",
    "                    std=torch.ones((num_samples, self.input_size)),\n",
    "                )\n",
    "                samples.append(self._inverse(z, cond=c * torch.ones(num_samples)))\n",
    "        else:\n",
    "            z = torch.normal(\n",
    "                mean=torch.zeros((num_samples, self.input_size)),\n",
    "                std=torch.ones((num_samples, self.input_size)),\n",
    "            )\n",
    "            samples.append(self._inverse(z, cond=cond * torch.ones(num_samples)))\n",
    "        return torch.cat(samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cinn(model, batchsize=1000, epochs=1000, lr=0.001): \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr) \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        x_data, y_data = #TODO\n",
    "        x_data, y_data = torch.Tensor(x_data), torch.Tensor(y_data)\n",
    "        \n",
    "        z, ljd = model(x_data, y_data)\n",
    "        \n",
    "        loss = torch.sum(0.5*torch.sum(z**2, -1)-ljd) / batchsize\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % (epochs//3) == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
