{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPip3HVYJemh+yP7o/oCe6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelkanthrawat/GNN-exercises/blob/main/Project/AE_mnist_superresolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "yVoYyJSYlgd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Bx09pzLnlGhs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Resize\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoencoderSimple(pl.LightningModule):\n",
        "    def __init__(self, encoder_linear_size=64):\n",
        "        super(AutoencoderSimple, self).__init__()\n",
        "\n",
        "        # Define the criterion\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_conv1 = nn.Conv2d(\n",
        "            1, 6, kernel_size=3, stride=2, padding=1\n",
        "        )  # Use stride 2 for downscaling\n",
        "        self.encoder_conv2 = nn.Conv2d(\n",
        "            6, 12, kernel_size=3, stride=2, padding=1\n",
        "        )  # Use stride 2 for downscaling\n",
        "        self.encoder_linear = nn.Linear(12 * 7 * 7, encoder_linear_size)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder_linear = nn.Linear(encoder_linear_size, 12 * 7 * 7)\n",
        "        self.decoder_conv1 = nn.ConvTranspose2d(\n",
        "            12, 6, kernel_size=3, stride=2, padding=1, output_padding=1\n",
        "        )  # Use stride 2 for upscaling\n",
        "        self.decoder_conv2 = nn.ConvTranspose2d(\n",
        "            6, 1, kernel_size=3, stride=2, padding=1, output_padding=1\n",
        "        )  # Use stride 2 for upscaling\n",
        "\n",
        "    def encoder(self, x):\n",
        "        # Encoder\n",
        "        x = self.encoder_conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.encoder_conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = x.view(-1, 12 * 7 * 7)\n",
        "        x = self.encoder_linear(x)\n",
        "        return x\n",
        "\n",
        "    def decoder(self, x):\n",
        "        # Decoder\n",
        "        x = self.decoder_linear(x)\n",
        "        x = x.view(-1, 12, 7, 7)\n",
        "        x = self.decoder_conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.decoder_conv2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, _ = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.criterion(outputs, images)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, _ = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.criterion(outputs, images)\n",
        "        self.log(\"val_loss\", loss)\n"
      ],
      "metadata": {
        "id": "pg10s-iIlVMP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the desired resolution\n",
        "desired_resolution = (10, 10)  # Change this to your desired resolution\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    Resize(desired_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset with the defined transformations\n",
        "mnist_reduced_size = MNIST(root='.', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "IdWrwwDgm3Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if pixel values are between 0 and 1\n",
        "for i in range(len(mnist_reduced_size)):\n",
        "    image, _ = mnist_reduced_size[i]\n",
        "    if (image.min() < 0) or (image.max() > 1):\n",
        "        print(\"Pixel values are not in the range [0, 1]\")\n",
        "        break\n",
        "else:\n",
        "    print(\"Pixel values are in the range [0, 1]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvOmoT0isWIz",
        "outputId": "39ffc475-2ced-4898-c830-1c713e077e87"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pixel values are in the range [0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### plotting a  few images\n",
        "# Extract 4 random images from the dataset\n",
        "random_indices = np.random.randint(0, len(mnist_reduced_size), size=4)\n",
        "images = [mnist_reduced_size[i][0] for i in random_indices]\n",
        "\n",
        "# Plot the images\n",
        "fig, axes = plt.subplots(1,4 , figsize=(8, 4))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(images[i][0], cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "RVspyjbVpVr1",
        "outputId": "6b680152-f329-4874-f4ab-a55014013269"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataloaders"
      ],
      "metadata": {
        "id": "3jBB8F2Co9sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and validation sets\n",
        "train_indices, val_indices = train_test_split(range(len(mnist_reduced_size)), test_size=0.2, random_state=42)\n",
        "\n",
        "# Create train and validation subsets\n",
        "train_dataset = Subset(mnist_reduced_size, train_indices)\n",
        "val_dataset = Subset(mnist_reduced_size, val_indices)\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = MNIST(root='.', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "on2orKo4o1vS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of datasets\n",
        "print(\"Train dataset shape:\", len(train_dataset))\n",
        "print(\"Validation dataset shape:\", len(val_dataset))\n",
        "print(\"Test dataset shape:\", len(test_dataset))\n",
        "\n",
        "# Extract one data point from each dataset\n",
        "train_sample = train_dataset[0]  # Extract the first data point from the train dataset\n",
        "val_sample = val_dataset[0]  # Extract the first data point from the validation dataset\n",
        "test_sample = test_dataset[0]  # Extract the first data point from the test dataset\n",
        "\n",
        "train_sample[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly254TkEpmhn",
        "outputId": "b982124f-46fb-414d-ce61-230633e695ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape: 48000\n",
            "Validation dataset shape: 12000\n",
            "Test dataset shape: 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the datasets\n",
        "torch.save(train_dataset, 'train_dataset_mnist_low_res.pth')\n",
        "torch.save(val_dataset, 'val_dataset_mnist_low_res.pth')\n",
        "torch.save(test_dataset, 'test_dataset_mnist_low_res.pth')"
      ],
      "metadata": {
        "id": "Ao9AZB6Qqd-Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "cfenI1Y-pAVG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code for training and validation"
      ],
      "metadata": {
        "id": "h8UOt2f2uq1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, train_loader, val_loader, print_epoch=1, num_epochs=10):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            loss = model.training_step(batch, batch_idx)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        if epoch % print_epoch == 0:\n",
        "            print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                loss = model.validation_step(batch, batch_idx)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if epoch % print_epoch == 0:\n",
        "            print(f\"Epoch {epoch}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n"
      ],
      "metadata": {
        "id": "yKYwEfzZsrEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}