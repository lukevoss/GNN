{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgecWFg84qA57UgqpNpRNX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelkanthrawat/GNN-exercises/blob/main/Assignment_1_Neel's_attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LgoPLanS-CL-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from numpy.linalg import norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### define a gaussian function in n_star dimn\n",
        "\n",
        "####"
      ],
      "metadata": {
        "id": "1JZb9EOS-K7r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### define two dimensional gaussian distribution\n",
        "\n",
        "### this gaussian has the ability to infer (evaluate the probability at the given point)\n",
        "def gaussian_2d(x:np.array,mean:np.array,cov_mat:np.array):\n",
        "  '''\n",
        "  This model will evaluate the probabilty at the given x_vector.\n",
        "  Args:\n",
        "  1. x= np array of shape (1 X dimn_of_the_problem=2)\n",
        "  2. mean= np array of shape (1 X dimn_of_the_problem=2)\n",
        "  3.cov_mat= square np array of shape (dimn_of_the_prob=2)\n",
        "  '''\n",
        "  dimn=np.shape(x)[0]\n",
        "  # x=np.array(x).reshape((1,dimn))\n",
        "  # mean=np.array(mean).reshape((1,dimn))\n",
        "  # print(\"shape(x),shape(mean)\")\n",
        "  # print(np.shape(x), np.shape(mean))\n",
        "  # note that x is a row vector\n",
        "\n",
        "  inv_cov=np.linalg.inv(cov_mat)\n",
        "  x_minus_mu=x-mean\n",
        "\n",
        "  # argument for the exp\n",
        "  arg_for_expn=0.5*(x_minus_mu @ inv_cov @ np.transpose(x_minus_mu))[0,0]\n",
        "  det_cov=np.linalg.det(cov_mat)\n",
        "  # evaluating the prob\n",
        "  prob=np.exp(-arg_for_expn) * (1./(2*np.pi*np.sqrt(det_cov)))\n",
        "\n",
        "  return prob"
      ],
      "metadata": {
        "id": "GH3pUOYhAg5C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([[0.5,0.5]])\n",
        "mean=np.array([[1,1]])\n",
        "cov_mat=np.array([[0.3,0.22],[0.22,0.8]])\n",
        "check=gaussian_2d(x,mean,cov_mat)\n",
        "print(check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0Gj7DOuJZ9P",
        "outputId": "33cc78e5-e132-4a0f-dbee-2eb0490d584d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2363860723405543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fitting the gaussian model\n",
        "###  when we fit the model, we need some loss function to optimize to find the parameters, is'nt it"
      ],
      "metadata": {
        "id": "rKyjRZ93Njhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My model should have 2 functionalities:\n",
        "1. inference ability (done temporarily)\n",
        "2. generation ability"
      ],
      "metadata": {
        "id": "xv_hzQBrDbKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Gaussian mixture model (GMM)"
      ],
      "metadata": {
        "id": "IkIg3qbfLUAb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZ8SxAwQLTYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EM algorithm for training the params for the GMM"
      ],
      "metadata": {
        "id": "Az3CW5-AQsdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.formatters import Dict\n",
        "def EM_training(mean:np.array,mixing_coeffs:np.array,covs:Dict, dataset_input:np.array):\n",
        "  '''\n",
        "  Args:\n",
        "  mean=\n",
        "        shape(mean)= (L X dimension_of_the_problem_instance (whether 1D, 2D, 3D and so on))\n",
        "  mixing_coeffs=\n",
        "        shape(mixing_coeffs)= (1 X L)\n",
        "  covs= dict of covariance matrices.\n",
        "        shape(covs)= L number of entries, one for each of the P_l(x)\n",
        "  dataset_input: shape would be (num of data set X dimn_of_the_prob)\n",
        "  '''\n",
        "  L=np.shape(mean)[0]\n",
        "  dimn_of_prob=np.shape(dataset_input)[1]\n",
        "  # define gamma: responsibility\n",
        "  num_data=np.shape(dataset_input)[0] ### note shape(dataset_input)=(num data X dimn_of_the_problem)\n",
        "  gamma=np.zeros(shape=(num_data,L))\n",
        "  num_iters=1 # denoted as T in notes\n",
        "  ###\n",
        "  for t in range(0,num_iters):\n",
        "\n",
        "    ### evaluate the E step\n",
        "    # update the gamma.\n",
        "    for idx_over_data in range(0,num_data):\n",
        "      ### here i need to evaluate the common denominator that will go inside the next loop\n",
        "      # print(\"covs[idx_el]\")\n",
        "      # print(covs[0])\n",
        "      prod_mixing_coeffs_and_gaussian_l=[mixing_coeffs[0,idx_el]*gaussian_2d(x=dataset_input[idx_over_data,:].reshape((1,dimn_of_prob)),mean=mean[idx_el,:].reshape((1,dimn_of_prob)),cov_mat=covs[idx_el]) for idx_el in range(0,L)]\n",
        "      # print(\"pil_Pl_fordfnwnf:\")\n",
        "      # print(prod_mixing_coeffs_and_gaussian_l)\n",
        "      Dr_for_gamma=np.sum(prod_mixing_coeffs_and_gaussian_l)\n",
        "      gamma[idx_over_data,:]=[prod_mixing_coeffs_and_gaussian_l[k]/Dr_for_gamma for k in range(0,L)]\n",
        "\n",
        "    print(\"gamma obtained:\");print(gamma)\n",
        "    ### M-step: Re-estimating the parameters\n",
        "    N_l=np.sum(gamma,axis=0)# this is the expression sum_{i=1}^{N(all dataset)} gamma_{il} in the lecture notes\n",
        "    print(\"N_l is:\");print(N_l)# NOTE:: shape of N_l = (L,) (note that this array is 1D)\n",
        "\n",
        "    ### 1. updating means: the expresion is like a wtd sum of the vectors\n",
        "    ###\n",
        "    for idx_el in range(0,L):\n",
        "      #print(\"index el is:\");print(idx_el)\n",
        "      temp_vec=np.array(\n",
        "          [gamma[idx_over_data,idx_el]*dataset_input[idx_over_data,:]\n",
        "           for idx_over_data in range(0,num_data)]\n",
        "          )\n",
        "      #print(\"temp vec is:\");print(temp_vec)\n",
        "      #update the mean\n",
        "      # print(\"np.sum(temp_vec,axis=0):\",np.sum(temp_vec,axis=0))\n",
        "      # print(\"mean[idx_el,:]=\",mean[idx_el,:])\n",
        "      mean[idx_el,:]=(1./N_l[idx_el])*np.sum(temp_vec,axis=0)\n",
        "    print(\"mean matrix after update:\");print(mean)\n",
        "\n",
        "    ### 2. update the covariance matrix:\n",
        "    ##### formula for updating the cov mat is:\n",
        "    ##### sigma_lth=1/N_l[idx_el] * sum_{i=1}^{N(all_dataset)} * gamma_{il}*some_outer_product\n",
        "    for idx_el in range(0,L):\n",
        "      #print(\"updating cov matrix, index el is:\",idx_el)\n",
        "      temp_outer_prod_list=np.array(\n",
        "          [gamma[idx_over_data,idx_el]*(dataset_input[idx_over_data,:]-mean[idx_el,:]).reshape((dimn_of_prob,1)) @\n",
        "           (dataset_input[idx_over_data,:]-mean[idx_el,:]).reshape((1,dimn_of_prob))\n",
        "          for idx_over_data in range(0,num_data)\n",
        "         ]\n",
        "          )\n",
        "      #print(\"temp outer prodcut list is:\");print(temp_outer_prod_list)\n",
        "      #print(\"summed over:\");print(sum(temp_outer_prod_list))\n",
        "      covs[idx_el]=(1./N_l[idx_el])*sum(temp_outer_prod_list)\n",
        "    print(\"updated covriances\");print(covs)\n",
        "\n",
        "    ### 3. update the mixing coeffs:\n",
        "    for idx_el in range(0,L):\n",
        "      mixing_coeffs[0,idx_el]=N_l[idx_el]/num_data\n",
        "    print(\"updated mixing_coeffs:\");print(mixing_coeffs)\n",
        "\n",
        "    return mean,covs,mixing_coeffs"
      ],
      "metadata": {
        "id": "NeuVPY6hQx-H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an example to check whether the function EM_training working or not.\n",
        "# L=4, N=3, dim_of_prob=2\n",
        "mean=np.array([[1.5,1.25],[0.75,1.25],[1.3,1.7],[1.4,1.63]])\n",
        "print(\"mean:\");print(mean)\n",
        "mixing_coeffs=np.array([[0.25,0.25,0.25,0.25]])\n",
        "covs={0:np.array([[0.02,0.003],[0.003,0.1]]),\n",
        "      1: np.array([[0.06,0.007],[0.007,0.05]]),\n",
        "      2:np.array([[0.068,0.003],[0.007,0.05]]),\n",
        "      3:np.array([[0.06,0.007],[0.017,0.078]])}\n",
        "dataset_input=np.array([[1.78,1],[1.75,1.3],[1.03,1.08]])\n",
        "checking_em=EM_training(mean=mean,\n",
        "                        mixing_coeffs=mixing_coeffs,\n",
        "                        covs=covs,\n",
        "                        dataset_input=dataset_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUW3xhLmp_fK",
        "outputId": "38fc1e83-16c3-44dd-ac4b-cf505ae6e9c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean:\n",
            "[[1.5  1.25]\n",
            " [0.75 1.25]\n",
            " [1.3  1.7 ]\n",
            " [1.4  1.63]]\n",
            "gamma obtained:\n",
            "[[9.18297348e-01 2.95287972e-04 6.13499528e-03 7.52723686e-02]\n",
            " [6.55581269e-01 5.90691003e-04 8.22543748e-02 2.61573665e-01]\n",
            " [1.10155835e-02 8.15573224e-01 3.47270990e-02 1.38684093e-01]]\n",
            "N_l is:\n",
            "[1.5848942  0.8164592  0.12311647 0.47553013]\n",
            "mean matrix after update:\n",
            "[[1.76237792 1.12464909]\n",
            " [1.03079216 1.08013023]\n",
            " [1.54840665 1.222996  ]\n",
            " [1.54476721 1.18835153]]\n",
            "updated covriances\n",
            "{0: array([[ 0.00397132, -0.00194324],\n",
            "       [-0.00194324,  0.02173501]]), 1: array([[5.77862753e-04, 9.27957540e-05],\n",
            "       [9.27957540e-05, 3.73140998e-05]]), 2: array([[0.10562853, 0.02870745],\n",
            "       [0.02870745, 0.0122072 ]]), 3: array([[0.10920867, 0.02185739],\n",
            "       [0.02185739, 0.01589628]])}\n",
            "updated mixing_coeffs:\n",
            "[[0.52829807 0.27215307 0.04103882 0.15851004]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a=np.array([[10,20],[1,2],[111,5]])\n",
        "# np.sum(a,axis=0),np.sum(a,axis=1)"
      ],
      "metadata": {
        "id": "7gzHwxQG2WUi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel density estimation.\n",
        "\n",
        "Note that the kernel density estimation model is a non-parametric model, hence it does not require any \"training\" so to speak!\n",
        "\n",
        "\n",
        "* discusses how to choose an appropriate bandwidth parameter: https://stats.libretexts.org/Bookshelves/Computing_and_Modeling/Supplemental_Modules_(Computing_and_Modeling)/Regression_Analysis/Nonparametric_Inference_-_Kernel_Density_Estimation#:~:text=Contributors-,Introduction%20and%20definition,%E2%88%88%CE%98%E2%8A%82Rd%7D.\n",
        "\n",
        "1. Squared exponential kernel:\n",
        "https://www.mathworks.com/help/stats/kernel-covariance-function-options.html"
      ],
      "metadata": {
        "id": "3_KVEjPXWr6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### defining kernel's required for the assingment.\n",
        "\n",
        "1. Squared exponential kernel\n",
        "2. Inverse-multi quadratic kernel"
      ],
      "metadata": {
        "id": "bTarhmv8-s_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Squared exponential kernel:"
      ],
      "metadata": {
        "id": "dL-wQFZ2-8PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=np.array([[1,2,3]]); b=np.array([[10,20,30]])\n",
        "norm(a-b)**2, a@a.T+b@b.T-2*a@b.T #obviously!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lte2rvdMAKbQ",
        "outputId": "d8ba11da-3d47-411f-c2bc-e051b2d71805"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1134.0, array([[1134]]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def squared_exp_kernel(xa:np.array, xb:np.array,bandwidth:float,amplitude:float=1):\n",
        "  '''\n",
        "  xa=row vector of shape (1xdimn_of_the_prob)\n",
        "  xb= row vector of shape (1X dimn_of_the_prob)\n",
        "  bandwidth: a hyperparameter\n",
        "  amplitude: overall variance (set to 1 in the lecture, can be changed as well)\n",
        "  '''\n",
        "  diff=xa-xb\n",
        "  kernel=(amplitude**2) *  np.exp( -((norm(diff))**2)/(2*bandwidth) )\n",
        "  return kernel"
      ],
      "metadata": {
        "id": "2wCor1FPWqua"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_multi_quadratic_kernel(xa:np.array, xb:np.array,bandwidth:float):\n",
        "  '''\n",
        "  xa=row vector of shape (1xdimn_of_the_prob)\n",
        "  xb= row vector of shape (1X dimn_of_the_prob)\n",
        "  bandwidth: a hyperparameter\n",
        "  '''\n",
        "  diff=xa-xb\n",
        "  kernel=bandwidth/(bandwidth+ ((norm(diff))**2) )\n",
        "  return kernel"
      ],
      "metadata": {
        "id": "82WryFfJBtTo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel_density_estimator(x:np.array, dataset:np.array, bandwidth:float, kernel_fn):\n",
        "  '''\n",
        "  x: a row vector (1X dimn of the problem) at which one wants to evaluate the KDE (prob function)\n",
        "  dataset= an array of shape (N X dimn of the problem)\n",
        "  bandwidth: hyperparameter\n",
        "  kernel_fn: choice of the user\n",
        "  '''\n",
        "  num_data=np.shape(dataset)[0]\n",
        "  list_kernel=[kernel_fn(xa=x,xb=dataset[j,:],bandwidth=bandwidth) for j in range(0,num_data)]\n",
        "  prob=(1./num_data)*sum(list_kernel)## sometimes instead of 1./num_data , we also have 1./(bandwidth X num_data)\n",
        "  return prob"
      ],
      "metadata": {
        "id": "5Ef7jzCFC6_i"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[10.5,-3.2,7.8]\n",
        "sum(a), 10.5-3.2+7.8, np.sum(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0I0melnKE-5",
        "outputId": "95c9bc3b-3a1f-49b5-a66c-c1dfcde55747"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15.1, 15.1, 15.1)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of MMD metric:\n",
        "\n",
        "a good resource for MMD: https://www.onurtunali.com/ml/2019/03/08/maximum-mean-discrepancy-in-machine-learning.html\n",
        "\n",
        "(implemented using pytorch)"
      ],
      "metadata": {
        "id": "jUiuhqZxR6nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXEELMrQR4ch"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}