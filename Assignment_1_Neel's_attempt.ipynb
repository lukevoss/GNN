{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwDJ3FqQVbSi3qQo2rm7ho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelkanthrawat/GNN-exercises/blob/main/Assignment_1_Neel's_attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LgoPLanS-CL-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from numpy.linalg import norm\n",
        "from IPython.core.formatters import Dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### define (its not just two) n- dimensional gaussian distribution\n",
        "### isnt this a general gaussian distribution! yes it is. I am a blithering idiot i must say!\n",
        "\n",
        "### this gaussian has the ability to infer (evaluate the probability at the given point)\n",
        "def gaussian_multivar(x:np.array,mean:np.array,cov_mat:np.array):\n",
        "  '''\n",
        "  This model will evaluate the probabilty at the given x_vector using the GMM model.\n",
        "  Args:\n",
        "  1. x= np array of shape (1 X dimn_of_the_problem)\n",
        "  2. mean= np array of shape (1 X dimn_of_the_problem)\n",
        "  3.cov_mat= square np array of shape (dimn_of_the_prob)\n",
        "  '''\n",
        "  dimn=np.shape(x)[0]\n",
        "  inv_cov=np.linalg.inv(cov_mat)\n",
        "  x_minus_mu=x-mean\n",
        "  # argument for the exp\n",
        "  arg_for_expn=0.5*(x_minus_mu @ inv_cov @ np.transpose(x_minus_mu))[0,0]\n",
        "  det_cov=np.linalg.det(cov_mat)\n",
        "  # evaluating the prob\n",
        "  prob=np.exp(-arg_for_expn) * (1./(np.sqrt(det_cov*((2*np.pi)**dimn))))\n",
        "\n",
        "  return prob"
      ],
      "metadata": {
        "id": "GH3pUOYhAg5C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([[0.5,0.5]])\n",
        "mean=np.array([[1,1]])\n",
        "cov_mat=np.array([[0.3,0.22],[0.22,0.8]])\n",
        "check=gaussian_multivar(x,mean,cov_mat)\n",
        "print(check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0Gj7DOuJZ9P",
        "outputId": "7ef7245e-e623-4b40-8144-a9a98e363fa6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5925320126578024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fitting the gaussian model\n",
        "###  when we fit the model, we need some loss function to optimize to find the parameters, is'nt it"
      ],
      "metadata": {
        "id": "rKyjRZ93Njhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My model should have 2 functionalities:\n",
        "1. inference ability (done temporarily)\n",
        "2. generation ability"
      ],
      "metadata": {
        "id": "xv_hzQBrDbKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Gaussian mixture model (GMM)"
      ],
      "metadata": {
        "id": "IkIg3qbfLUAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A note for me:\n",
        "\n",
        "For the time being, just for the sake of having a working code ready at my disposal, I have encoded the covs as a dictionary. maybe it can be encoded as a tensor as well."
      ],
      "metadata": {
        "id": "Ns8PO7TjNFIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_mixture_model(x_input:np.array, mixing_coeffs:np.array, mean_array:np.array, covs:Dict):\n",
        "  '''\n",
        "  Args:\n",
        "  1. x_input: x vector at which one wants to evaluate the GMM model.\n",
        "              A numpy array (row vector) of shape (1 X dimn of the problem)\n",
        "  2. mean:  A numpy arrray of shape (L X dimn of the problem instance)\n",
        "  3. mixing coeffs: a numpy array of shape (1 X L)\n",
        "  4. covs: dict of covaraince matrices, L number of entries, one for each of the P_l(x).\n",
        "  '''\n",
        "  L=np.shape(mean)[0]\n",
        "  dimn_of_the_prob=np.shape(x_input)[1]\n",
        "  list_gmm=[mixing_coeffs[1,idx_el] * gaussian_multivar(x=x_input, mean=mean[idx_el,:],cov_mat=covs[idx_el])\n",
        "            for idx_el in range(0,L)\n",
        "            ]\n",
        "  prob=sum(list_gmm)## sometimes instead of 1./num_data , literature uses the factor 1./(bandwidth X num_data)\n",
        "  return prob"
      ],
      "metadata": {
        "id": "lZ8SxAwQLTYx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EM algorithm for training the params for the GMM"
      ],
      "metadata": {
        "id": "Az3CW5-AQsdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EM_training(mean:np.array,mixing_coeffs:np.array,covs:Dict, dataset_input:np.array,num_iters:int=100,tol:float=0.0001):\n",
        "  '''\n",
        "  Args:\n",
        "  mean= a numpy array\n",
        "        shape(mean)= (L X dimension_of_the_problem_instance (whether 1D, 2D, 3D and so on))\n",
        "  mixing_coeffs=\n",
        "        shape(mixing_coeffs)= (1 X L)\n",
        "  covs= dict of covariance matrices.\n",
        "        shape(covs)= L number of entries, one for each of the P_l(x)\n",
        "  dataset_input: shape would be (num of data set X dimn_of_the_prob)\n",
        "  '''\n",
        "  L=np.shape(mean)[0]\n",
        "  dimn_of_prob=np.shape(dataset_input)[1]\n",
        "  # define gamma: responsibility\n",
        "  num_data=np.shape(dataset_input)[0] ### note shape(dataset_input)=(num data X dimn_of_the_problem)\n",
        "  gamma=np.zeros(shape=(num_data,L))\n",
        "  prev_log_likelihood = -np.inf  # Initialize the log-likelihood to negative infinity\n",
        "  ###\n",
        "  for t in range(0,num_iters):\n",
        "\n",
        "    ### evaluate the E step\n",
        "    # update the gamma.\n",
        "    for idx_over_data in range(0,num_data):\n",
        "      prod_mixing_coeffs_and_gaussian_l=[mixing_coeffs[0,idx_el]*gaussian_multivar(x=dataset_input[idx_over_data,:].reshape((1,dimn_of_prob)),mean=mean[idx_el,:].reshape((1,dimn_of_prob)),cov_mat=covs[idx_el]) for idx_el in range(0,L)]\n",
        "      Dr_for_gamma=np.sum(prod_mixing_coeffs_and_gaussian_l)\n",
        "      gamma[idx_over_data,:]=[prod_mixing_coeffs_and_gaussian_l[k]/Dr_for_gamma for k in range(0,L)]\n",
        "\n",
        "    #print(\"gamma obtained:\");print(gamma)\n",
        "    ### M-step: Re-estimating the parameters\n",
        "    N_l=np.sum(gamma,axis=0)# this is the expression sum_{i=1}^{N(all dataset)} gamma_{il} in the lecture notes\n",
        "    #print(\"N_l is:\");print(N_l)# NOTE:: shape of N_l = (L,) (note that this array is 1D)\n",
        "\n",
        "    ### 1. update means: the expresion is like a wtd sum of the vectors\n",
        "    for idx_el in range(0,L):\n",
        "      #print(\"index el is:\");print(idx_el)\n",
        "      temp_vec=np.array(\n",
        "          [gamma[idx_over_data,idx_el]*dataset_input[idx_over_data,:]\n",
        "           for idx_over_data in range(0,num_data)]\n",
        "          )\n",
        "      mean[idx_el,:]=(1./N_l[idx_el])*np.sum(temp_vec,axis=0)\n",
        "    #print(\"mean matrix after update:\");print(mean)\n",
        "\n",
        "    ### 2. update the covariance matrix:\n",
        "    ##### formula for updating the cov mat is:\n",
        "    ##### sigma_lth=1/N_l[idx_el] * sum_{i=1}^{N(all_dataset)} * gamma_{il}*some_outer_product\n",
        "    for idx_el in range(0,L):\n",
        "      #print(\"updating cov matrix, index el is:\",idx_el)\n",
        "      temp_outer_prod_list=np.array(\n",
        "          [gamma[idx_over_data,idx_el]*(dataset_input[idx_over_data,:]-mean[idx_el,:]).reshape((dimn_of_prob,1)) @\n",
        "           (dataset_input[idx_over_data,:]-mean[idx_el,:]).reshape((1,dimn_of_prob))\n",
        "          for idx_over_data in range(0,num_data)\n",
        "         ]\n",
        "          )\n",
        "      covs[idx_el]=(1./N_l[idx_el])*sum(temp_outer_prod_list)\n",
        "    #print(\"updated covriances\");print(covs)\n",
        "\n",
        "    ### 3. update the mixing coeffs:\n",
        "    for idx_el in range(0,L):\n",
        "      mixing_coeffs[0,idx_el]=N_l[idx_el]/num_data\n",
        "\n",
        "    # Calculate the log-likelihood for the current parameters\n",
        "    current_log_likelihood = 0\n",
        "    for idx_over_data in range(num_data):\n",
        "        log_sum = np.log(sum([mixing_coeffs[0, idx_el] * gaussian_multivar(x=dataset_input[idx_over_data, :].reshape((1, dimn_of_prob)), mean=mean[idx_el, :].reshape((1, dimn_of_prob)), cov_mat=covs[idx_el]) for idx_el in range(L)]))\n",
        "        current_log_likelihood += log_sum\n",
        "\n",
        "    # Check for convergence\n",
        "    if np.abs(current_log_likelihood - prev_log_likelihood) < tol:\n",
        "        break\n",
        "\n",
        "    prev_log_likelihood = current_log_likelihood\n",
        "\n",
        "  return mean,covs,mixing_coeffs"
      ],
      "metadata": {
        "id": "NeuVPY6hQx-H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an example to check whether the function EM_training working or not.\n",
        "# L=2, N=3, dim_of_prob=2\n",
        "mean=np.array([[0.9,0.8],[4,4]])#np.array([[1.5,1.25],[0.75,1.25],[1.3,1.7],[1.4,1.63]])\n",
        "print(\"mean:\");print(mean)\n",
        "mixing_coeffs=np.array([[0.5,0.5]])\n",
        "covs={0:np.array([[1,0.5],[0.5,1.0]]),\n",
        "      1: np.array([[1,-0.5],[-0.5,1]])}\n",
        "data_1=np.random.multivariate_normal(mean[0,:], covs[0], int(100 * mixing_coeffs[0,0]))\n",
        "data_2= np.random.multivariate_normal(mean[1,:], covs[1], int(100 * mixing_coeffs[0,1]))\n",
        "dataset_input=np.vstack((data_1, data_2))\n",
        "updated_mean, updated_cos, updated_mix_coeffs=EM_training(mean=mean,\n",
        "                        mixing_coeffs=mixing_coeffs,\n",
        "                        covs=covs,\n",
        "                        dataset_input=dataset_input)\n",
        "updated_mean, updated_cos, updated_mix_coeffs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUW3xhLmp_fK",
        "outputId": "3b9fc0f9-e2f0-4fe2-a1a2-2bbaede52cf6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean:\n",
            "[[0.9 0.8]\n",
            " [4.  4. ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.6933802 , 0.74956438],\n",
              "        [3.93796468, 4.11063742]]),\n",
              " {0: array([[1.08133765, 0.56254151],\n",
              "         [0.56254151, 0.86965017]]),\n",
              "  1: array([[ 1.1452362 , -0.38999791],\n",
              "         [-0.38999791,  0.8591685 ]])},\n",
              " array([[0.50402411, 0.49597589]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a=np.array([[10,20],[1,2],[111,5]])\n",
        "# np.sum(a,axis=0),np.sum(a,axis=1)"
      ],
      "metadata": {
        "id": "7gzHwxQG2WUi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel density estimation.\n",
        "\n",
        "Note that the kernel density estimation model is a non-parametric model, hence it does not require any \"training\" so to speak!\n",
        "\n",
        "\n",
        "* discusses how to choose an appropriate bandwidth parameter: https://stats.libretexts.org/Bookshelves/Computing_and_Modeling/Supplemental_Modules_(Computing_and_Modeling)/Regression_Analysis/Nonparametric_Inference_-_Kernel_Density_Estimation#:~:text=Contributors-,Introduction%20and%20definition,%E2%88%88%CE%98%E2%8A%82Rd%7D.\n",
        "\n",
        "1. Squared exponential kernel:\n",
        "https://www.mathworks.com/help/stats/kernel-covariance-function-options.html"
      ],
      "metadata": {
        "id": "3_KVEjPXWr6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### defining kernel's required for the assingment.\n",
        "\n",
        "1. Squared exponential kernel\n",
        "2. Inverse-multi quadratic kernel"
      ],
      "metadata": {
        "id": "bTarhmv8-s_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Squared exponential kernel:"
      ],
      "metadata": {
        "id": "dL-wQFZ2-8PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a=np.array([[1,2,3]]); b=np.array([[10,20,30]]) # just checking something\n",
        "# norm(a-b)**2, a@a.T+b@b.T-2*a@b.T #they are the same, obviously! duh!"
      ],
      "metadata": {
        "id": "lte2rvdMAKbQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def squared_exp_kernel(xa:np.array, xb:np.array,bandwidth:float,amplitude:float=1):\n",
        "  '''\n",
        "  xa=row vector of shape (1xdimn_of_the_prob)\n",
        "  xb= row vector of shape (1X dimn_of_the_prob)\n",
        "  bandwidth: a hyperparameter\n",
        "  amplitude: overall variance (set to 1 in the lecture, can be changed as well)\n",
        "  '''\n",
        "  diff=xa-xb\n",
        "  kernel=(amplitude**2) *  np.exp( -((norm(diff))**2)/(2*bandwidth) )\n",
        "  return kernel"
      ],
      "metadata": {
        "id": "2wCor1FPWqua"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_multi_quadratic_kernel(xa:np.array, xb:np.array,bandwidth:float):\n",
        "  '''\n",
        "  xa=row vector of shape (1xdimn_of_the_prob)\n",
        "  xb= row vector of shape (1X dimn_of_the_prob)\n",
        "  bandwidth: a hyperparameter\n",
        "  '''\n",
        "  diff=xa-xb\n",
        "  kernel=bandwidth/(bandwidth+ ((norm(diff))**2) )\n",
        "  return kernel"
      ],
      "metadata": {
        "id": "82WryFfJBtTo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel_density_estimator(x:np.array, dataset:np.array, bandwidth:float, kernel_fn):\n",
        "  '''\n",
        "  x: a row vector (1X dimn of the problem) at which one wants to evaluate the KDE (prob function)\n",
        "  dataset= an array of shape (N X dimn of the problem)\n",
        "  bandwidth: hyperparameter\n",
        "  kernel_fn: choice of the user\n",
        "  '''\n",
        "  num_data=np.shape(dataset)[0]\n",
        "  list_kernel=[kernel_fn(xa=x,xb=dataset[j,:],bandwidth=bandwidth) for j in range(0,num_data)]\n",
        "  prob=(1./num_data)*sum(list_kernel)## sometimes instead of 1./num_data , we also have 1./(bandwidth X num_data)\n",
        "  return prob"
      ],
      "metadata": {
        "id": "5Ef7jzCFC6_i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[10.5,-3.2,7.8]\n",
        "sum(a), 10.5-3.2+7.8, np.sum(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0I0melnKE-5",
        "outputId": "46b88d3c-e2fd-4e2e-b0b4-19586326c728"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15.1, 15.1, 15.1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of MMD metric:\n",
        "\n",
        "a good resource for MMD: https://www.onurtunali.com/ml/2019/03/08/maximum-mean-discrepancy-in-machine-learning.html\n",
        "\n",
        "(implemented using pytorch)"
      ],
      "metadata": {
        "id": "jUiuhqZxR6nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXEELMrQR4ch"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}